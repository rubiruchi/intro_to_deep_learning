{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Conv Net\n",
    "\n",
    "We'll use the MNIST dataset to create a conv net to achieve 99% accuracy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import time\n",
    "from datetime import timedelta\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conv layer 1\n",
    "filter_size1 = 5\n",
    "num_filters1 = 16\n",
    "\n",
    "# Conv layer 2\n",
    "filter_size2 = 5\n",
    "num_filters2 = 32\n",
    "\n",
    "# Fully connected layer\n",
    "fc_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST/train-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST/train-labels-idx1-ubyte.gz\n",
      "Extracting data/MNIST/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Load our data from tensorflow\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "data = input_data.read_data_sets('data/MNIST/', one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of:\n",
      "- Training set: 55000\n",
      "- Test set: 10000\n",
      "- Validation set: 5000\n"
     ]
    }
   ],
   "source": [
    "print(\"Size of:\")\n",
    "print(\"- Training set: {}\".format(len(data.train.labels)))\n",
    "print(\"- Test set: {}\".format(len(data.test.labels)))\n",
    "print(\"- Validation set: {}\".format(len(data.validation.labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# More hypes\n",
    "\n",
    "img_size = 28\n",
    "\n",
    "# Images in one dimensional arrays\n",
    "img_size_flat = img_size * img_size\n",
    "\n",
    "img_shape = (img_size, img_size)\n",
    "\n",
    "num_channels = 1 # Because we're using greyscale numbers\n",
    "\n",
    "num_classes = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_images(images, cls_true, cls_pred=None):\n",
    "    assert len(images) == len(cls_true) == 9\n",
    "    \n",
    "    # Create figure with 3x3 sub-plots\n",
    "    fig, axes = plt.subplots(3,3)\n",
    "    fig.subplots_adjust(hspace=0.3, wspace=0.3)\n",
    "    \n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        # plot image\n",
    "        ax.imshow(images[i].reshape(img_shape), cmap='binary')\n",
    "        \n",
    "        # Show true and predicted classes.\n",
    "        if cls_pred is None:\n",
    "            print(cls_true[i])\n",
    "            xlabel = \"True: {0}\".format(cls_true[i])\n",
    "        else:\n",
    "            xlabel = \"True: {0}, Pred: {1}\".format(cls_true[i], cls_pred[i])\n",
    "\n",
    "        # Show the classes as the label on the x-axis.\n",
    "        ax.set_xlabel(xlabel)\n",
    "        \n",
    "        # Remove ticks from the plot.\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "    \n",
    "    # Ensure the plot is shown correctly with multiple plots\n",
    "    # in a single Notebook cell.\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.]\n",
      "[ 0.  0.  1.  0.  0.  0.  0.  0.  0.  0.]\n",
      "[ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "[ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "[ 0.  0.  0.  0.  1.  0.  0.  0.  0.  0.]\n",
      "[ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "[ 0.  0.  0.  0.  1.  0.  0.  0.  0.  0.]\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
      "[ 0.  0.  0.  0.  0.  1.  0.  0.  0.  0.]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAb8AAAD5CAYAAAC3ZTu3AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1063e2050>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get the first 10 images\n",
    "images = data.test.images[0:9]\n",
    "\n",
    "# Get the true classes for those images.\n",
    "cls_true = data.test.labels[0:9]\n",
    "\n",
    "# Plot the images and labels using our helper-function above.\n",
    "plot_images(images=images, cls_true=cls_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_weights(shape):\n",
    "    return tf.Variable(tf.truncated_normal(shape, stddev=0.05))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_biases(length):\n",
    "    return tf.Variable(tf.constant(0.05, shape=[length]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_conv_layer(input, num_input_channels, filter_size, num_filters, use_pooling=True):\n",
    "    # Shape of the filter-weights for the convolution.\n",
    "    # This format is determined by the TensorFlow API.\n",
    "    shape = [filter_size, filter_size, num_input_channels, num_filters]\n",
    "\n",
    "    # Create new weights for the filters with the given shape.\n",
    "    weights = new_weights(shape=shape)\n",
    "    \n",
    "    # Create new biases, one for each filter.\n",
    "    biases = new_biases(length=num_filters)\n",
    "    \n",
    "    # Create the TensorFlow operation for convolution.\n",
    "    # Note the strides are set to 1 in all dimensions.\n",
    "    # The first and last stride must always be 1,\n",
    "    # because the first is for the image-number and\n",
    "    # the last is for the input-channel.\n",
    "    # But e.g. strides=[1, 2, 2, 1] would mean that the filter\n",
    "    # is moved 2 pixels across the x- and y-axis of the image.\n",
    "    # The padding is set to 'SAME' which means the input image\n",
    "    # is padded with zeroes so the size of the output is the same.\n",
    "    layer = tf.nn.conv2d(input=input,\n",
    "                         filter=weights,\n",
    "                         strides=[1, 1, 1, 1],\n",
    "                         padding='SAME')\n",
    "    \n",
    "    # Add the biases to the results of the convolution.\n",
    "    # A bias-value is added to each filter-channel.\n",
    "    layer += biases\n",
    "    \n",
    "    if use_pooling:\n",
    "        # This is 2x2 max-pooling, which means that we\n",
    "        # consider 2x2 windows and select the largest value\n",
    "        # in each window. Then we move 2 pixels to the next window.\n",
    "        layer = tf.nn.max_pool(value=layer,\n",
    "                               ksize=[1, 2, 2, 1],\n",
    "                               strides=[1, 2, 2, 1],\n",
    "                               padding='SAME') \n",
    "        # Same means we pad our image with 0's so our output matrix will be the same size\n",
    "\n",
    "    # Rectified Linear Unit (ReLU).\n",
    "    # It calculates max(x, 0) for each input pixel x.\n",
    "    # This adds some non-linearity to the formula and allows us\n",
    "    # to learn more complicated functions.\n",
    "    layer = tf.nn.relu(layer)\n",
    "\n",
    "    # Note that ReLU is normally executed before the pooling,\n",
    "    # but since relu(max_pool(x)) == max_pool(relu(x)) we can\n",
    "    # save 75% of the relu-operations by max-pooling first.\n",
    "\n",
    "    # We return both the resulting layer and the filter-weights\n",
    "    # because we will plot the weights later.\n",
    "    return layer, weights\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_layer(layer):\n",
    "    # Get the shape of the input layer.\n",
    "    layer_shape = layer.get_shape()\n",
    "\n",
    "    # The shape of the input layer is assumed to be:\n",
    "    # layer_shape == [num_images, img_height, img_width, num_channels]\n",
    "\n",
    "    # The number of features is: img_height * img_width * num_channels\n",
    "    # We can use a function from TensorFlow to calculate this.\n",
    "    num_features = layer_shape[1:4].num_elements()\n",
    "    \n",
    "    # Reshape the layer to [num_images, num_features].\n",
    "    # Note that we just set the size of the second dimension\n",
    "    # to num_features and the size of the first dimension to -1\n",
    "    # which means the size in that dimension is calculated\n",
    "    # so the total size of the tensor is unchanged from the reshaping.\n",
    "    layer_flat = tf.reshape(layer, [-1, num_features])\n",
    "\n",
    "    # The shape of the flattened layer is now:\n",
    "    # [num_images, img_height * img_width * num_channels]\n",
    "\n",
    "    # Return both the flattened layer and the number of features.\n",
    "    return layer_flat, num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_fc_layer(input,          # The previous layer.\n",
    "                 num_inputs,     # Num. inputs from prev. layer.\n",
    "                 num_outputs,    # Num. outputs.\n",
    "                 use_relu=True): # Use Rectified Linear Unit (ReLU)?\n",
    "\n",
    "    # Create new weights and biases.\n",
    "    weights = new_weights(shape=[num_inputs, num_outputs])\n",
    "    biases = new_biases(length=num_outputs)\n",
    "\n",
    "    # Calculate the layer as the matrix multiplication of\n",
    "    # the input and weights, and then add the bias-values.\n",
    "    layer = tf.matmul(input, weights) + biases\n",
    "\n",
    "    # Use ReLU?\n",
    "    if use_relu:\n",
    "        layer = tf.nn.relu(layer)\n",
    "\n",
    "    return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, shape=[None, img_size_flat], name='x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_image = tf.reshape(x, [-1, img_size, img_size, num_channels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = tf.placeholder(tf.float32, shape=[None, 10], name='y_true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true_cls = tf.argmax(y_true, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_conv1, weights_conv1 = new_conv_layer(\n",
    "                                        input=x_image, num_input_channels=num_channels,\n",
    "                                        filter_size=filter_size1,\n",
    "                                        num_filters=num_filters1,\n",
    "                                        use_pooling=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Relu_1:0' shape=(?, 14, 14, 16) dtype=float32>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_conv1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_conv2, weights_conv2 = \\\n",
    "    new_conv_layer(input=layer_conv1,\n",
    "                   num_input_channels=num_filters1,\n",
    "                   filter_size=filter_size2,\n",
    "                   num_filters=num_filters2,\n",
    "                   use_pooling=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Relu_2:0' shape=(?, 7, 7, 32) dtype=float32>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_conv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_flat, num_features = flatten_layer(layer_conv2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Reshape_1:0' shape=(?, 1568) dtype=float32>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1568"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_fc1 = new_fc_layer(input=layer_flat,\n",
    "                         num_inputs=num_features,\n",
    "                         num_outputs=fc_size,\n",
    "                         use_relu=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Relu_3:0' shape=(?, 128) dtype=float32>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_fc1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_fc2 = new_fc_layer(input=layer_fc1,\n",
    "                         num_inputs=fc_size,\n",
    "                         num_outputs=num_classes,\n",
    "                         use_relu=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'add_4:0' shape=(?, 10) dtype=float32>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_fc2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use softmax to normalise our outputs to between 0-1 where the numbers for every class sum to 1.0 to represent a probability\n",
    "y_pred = tf.nn.softmax(layer_fc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The class is the index of the largest prediction\n",
    "y_pred_cls = tf.argmax(y_pred, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross entropy will be our loss, we'll use the output of fc2 directly, before we apply softmax\n",
    "# This calculates per image\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=layer_fc2,\n",
    "                                                        labels=y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll calc the average of our cross entropy loss\n",
    "cost = tf.reduce_mean(cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And use the adam optimizer to reduce the mean of cross entropy across our images\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=1e-4).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vector of booleans wether predicted class equals true class (true positives)\n",
    "correct_prediction = tf.equal(y_pred_cls, y_true_cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the accuracy\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch_size = 64\n",
    "\n",
    "session = tf.Session()\n",
    "\n",
    "session.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counter for total number of iterations performed so far.\n",
    "total_iterations = 0\n",
    "\n",
    "def optimize(num_iterations):\n",
    "    # Ensure we update the global variable rather than a local copy.\n",
    "    global total_iterations\n",
    "\n",
    "    # Start-time used for printing time-usage below.\n",
    "    start_time = time.time()\n",
    "\n",
    "    for i in range(total_iterations,\n",
    "                   total_iterations + num_iterations):\n",
    "\n",
    "        # Get a batch of training examples.\n",
    "        # x_batch now holds a batch of images and\n",
    "        # y_true_batch are the true labels for those images.\n",
    "        x_batch, y_true_batch = data.train.next_batch(train_batch_size)\n",
    "\n",
    "        # Put the batch into a dict with the proper names\n",
    "        # for placeholder variables in the TensorFlow graph.\n",
    "        feed_dict_train = {x: x_batch,\n",
    "                           y_true: y_true_batch}\n",
    "\n",
    "        # Run the optimizer using this batch of training data.\n",
    "        # TensorFlow assigns the variables in feed_dict_train\n",
    "        # to the placeholder variables and then runs the optimizer.\n",
    "        session.run(optimizer, feed_dict=feed_dict_train)\n",
    "\n",
    "        # Print status every 100 iterations.\n",
    "        if i % 100 == 0:\n",
    "            # Calculate the accuracy on the training-set.\n",
    "            acc = session.run(accuracy, feed_dict=feed_dict_train)\n",
    "\n",
    "            # Message for printing.\n",
    "            msg = \"Optimization Iteration: {0:>6}, Training Accuracy: {1:>6.1%}\"\n",
    "\n",
    "            # Print it.\n",
    "            print(msg.format(i + 1, acc))\n",
    "\n",
    "    # Update the total number of iterations performed.\n",
    "    total_iterations += num_iterations\n",
    "\n",
    "    # Ending time.\n",
    "    end_time = time.time()\n",
    "\n",
    "    # Difference between start and end-times.\n",
    "    time_dif = end_time - start_time\n",
    "\n",
    "    # Print the time-usage.\n",
    "    print(\"Time usage: \" + str(timedelta(seconds=int(round(time_dif)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_example_errors(cls_pred, correct):\n",
    "    # This function is called from print_test_accuracy() below.\n",
    "\n",
    "    # cls_pred is an array of the predicted class-number for\n",
    "    # all images in the test-set.\n",
    "\n",
    "    # correct is a boolean array whether the predicted class\n",
    "    # is equal to the true class for each image in the test-set.\n",
    "\n",
    "    # Negate the boolean array.\n",
    "    incorrect = (correct == False)\n",
    "    \n",
    "    # Get the images from the test-set that have been\n",
    "    # incorrectly classified.\n",
    "    images = data.test.images[incorrect]\n",
    "    \n",
    "    # Get the predicted classes for those images.\n",
    "    cls_pred = cls_pred[incorrect]\n",
    "\n",
    "    # Get the true classes for those images.\n",
    "    cls_true = data.test.cls[incorrect]\n",
    "    \n",
    "    # Plot the first 9 images.\n",
    "    plot_images(images=images[0:9],\n",
    "                cls_true=cls_true[0:9],\n",
    "                cls_pred=cls_pred[0:9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cls_pred):\n",
    "    # This is called from print_test_accuracy() below.\n",
    "\n",
    "    # cls_pred is an array of the predicted class-number for\n",
    "    # all images in the test-set.\n",
    "\n",
    "    # Get the true classifications for the test-set.\n",
    "    cls_true = data.test.cls\n",
    "    \n",
    "    # Get the confusion matrix using sklearn.\n",
    "    cm = confusion_matrix(y_true=cls_true,\n",
    "                          y_pred=cls_pred)\n",
    "\n",
    "    # Print the confusion matrix as text.\n",
    "    print(cm)\n",
    "\n",
    "    # Plot the confusion matrix as an image.\n",
    "    plt.matshow(cm)\n",
    "\n",
    "    # Make various adjustments to the plot.\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(num_classes)\n",
    "    plt.xticks(tick_marks, range(num_classes))\n",
    "    plt.yticks(tick_marks, range(num_classes))\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "\n",
    "    # Ensure the plot is shown correctly with multiple plots\n",
    "    # in a single Notebook cell.\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the test-set into smaller batches of this size.\n",
    "test_batch_size = 256\n",
    "\n",
    "def print_test_accuracy(show_example_errors=False,\n",
    "                        show_confusion_matrix=False):\n",
    "\n",
    "    # Number of images in the test-set.\n",
    "    num_test = len(data.test.images)\n",
    "\n",
    "    # Allocate an array for the predicted classes which\n",
    "    # will be calculated in batches and filled into this array.\n",
    "    cls_pred = np.zeros(shape=num_test, dtype=np.int)\n",
    "\n",
    "    # Now calculate the predicted classes for the batches.\n",
    "    # We will just iterate through all the batches.\n",
    "    # There might be a more clever and Pythonic way of doing this.\n",
    "\n",
    "    # The starting index for the next batch is denoted i.\n",
    "    i = 0\n",
    "\n",
    "    while i < num_test:\n",
    "        # The ending index for the next batch is denoted j.\n",
    "        j = min(i + test_batch_size, num_test)\n",
    "\n",
    "        # Get the images from the test-set between index i and j.\n",
    "        images = data.test.images[i:j, :]\n",
    "\n",
    "        # Get the associated labels.\n",
    "        labels = data.test.labels[i:j, :]\n",
    "\n",
    "        # Create a feed-dict with these images and labels.\n",
    "        feed_dict = {x: images,\n",
    "                     y_true: labels}\n",
    "\n",
    "        # Calculate the predicted class using TensorFlow.\n",
    "        cls_pred[i:j] = session.run(y_pred_cls, feed_dict=feed_dict)\n",
    "\n",
    "        # Set the start-index for the next batch to the\n",
    "        # end-index of the current batch.\n",
    "        i = j\n",
    "\n",
    "    # Convenience variable for the true class-numbers of the test-set.\n",
    "    cls_true = data.test.label\n",
    "\n",
    "    # Create a boolean array whether each image is correctly classified.\n",
    "    correct = (cls_true == cls_pred)\n",
    "\n",
    "    # Calculate the number of correctly classified images.\n",
    "    # When summing a boolean array, False means 0 and True means 1.\n",
    "    correct_sum = correct.sum()\n",
    "\n",
    "    # Classification accuracy is the number of correctly classified\n",
    "    # images divided by the total number of images in the test-set.\n",
    "    acc = float(correct_sum) / num_test\n",
    "\n",
    "    # Print the accuracy.\n",
    "    msg = \"Accuracy on Test-Set: {0:.1%} ({1} / {2})\"\n",
    "    print(msg.format(acc, correct_sum, num_test))\n",
    "\n",
    "    # Plot some examples of mis-classifications, if desired.\n",
    "    if show_example_errors:\n",
    "        print(\"Example errors:\")\n",
    "        plot_example_errors(cls_pred=cls_pred, correct=correct)\n",
    "\n",
    "    # Plot the confusion matrix, if desired.\n",
    "    if show_confusion_matrix:\n",
    "        print(\"Confusion Matrix:\")\n",
    "        plot_confusion_matrix(cls_pred=cls_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataSet' object has no attribute 'label'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-54-1bf4d7db47a1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint_test_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-53-24a08cd159d8>\u001b[0m in \u001b[0;36mprint_test_accuracy\u001b[0;34m(show_example_errors, show_confusion_matrix)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;31m# Convenience variable for the true class-numbers of the test-set.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m     \u001b[0mcls_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;31m# Create a boolean array whether each image is correctly classified.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataSet' object has no attribute 'label'"
     ]
    }
   ],
   "source": [
    "print_test_accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimize(num_iterations=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xoptimize(num_iterations=99)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
